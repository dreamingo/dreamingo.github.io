---
layout: post
title: "Object-Detection系列 - YOLOv2"
modified:
categories: 机器学习 
description: "Object-Detection Deep-Learning YOLO"
tags: [Object-Detection Deep-Learning YOLO]
comments: true
mathjax: true
share:
date: 2018-07-23T22:16:13+08:00
---

## 0x01. 前言

论文 [YOLO9000: Better, Faster, Stronger](https://arxiv.org/pdf/1612.08242.pdf) 也是 Joseph Redmon 大神在博士后期间的工作。作为 CVPR 2017 Best Paper Honorable Mention，该论文刚提出的时候也掀起过一阵阵讨论热潮，特别是对于 YOLO9000 模型新奇的训练技巧感到惊奇。

在学术界上，YOLO系列一直有着被低估的趋向，其低估的原因主要来源于 Darknet 系列相关文档少、社区活跃度低，因此无论是代码阅读、修改和实际上手起来会比较困难。另外也不得不惊叹下作者的工程能力出色，在基本无依赖于第三方框架的情况下，徒手撸出一个性能出色、速度快的检测系统。但是在工业届中，由于作者大无畏的`LICENSE.fuck`协议精神，很多公司会内部针对C源码做大量的二次开发。
<figure>
	<a href=""><img src="/assets/images/yolov2/license.png" alt="" width="500" heigh="200"></a>
    <figcaption><a href="" title="">DO WHAT THE FUCK YOU WANT TO PUBLIC LICENSE</a></figcaption>
</figure>

在该论文中，其实提出了两个目标检测模型：**YOLOv2** 和 **YOLO9000**

* **YOLOv2：** 模型是在YOLO的基础上，提出了大量的改进措施，在速度和精确度上都得到了很大的提升。
* **YOLO9000：** 论文中指出如今的目标检测模型都只能辨认少数类别的物体。YOLO9000模型是在YOLOv2的基础上，提出一种基于**检测数据**和**分类数据**联合训练的方法，使模型最终能够识别9000多种物体的分类。其利用：
    * 标记好的**检测数据**来精确的定位物体；
    * 利用大量的**分类数据**来协助分类和提高类别容量。

<figure>
	<a href=""><img src="/assets/images/yolov2/result.png" alt="" width="600" heigh="200"></a>
    <figcaption><a href="" title="">YOLOv2和其他目标检测算法在PASCAL VOC2007数据集上的效果对比</a></figcaption>
</figure>

## 0x02. YOLOv2

在这里，我主要介绍下， YOLOv2在YOLO的基础上，做了哪些改进 & Why。在开始之前，我们先通过论文中的一张从 YOLO 到 YOLOv2 的『优化路径图』，来对这些优化的技术和提升的幅度有个大致的了解。

<figure>
	<a href=""><img src="/assets/images/yolov2/path.png" alt="" width="700" heigh="200"></a>
</figure>

### 1. Batch Normalization
YOLOv2在所有的卷积层后都加入了 Batch Normalization层（一般位于ReLU层前，在`caffe`中，BN层分成了`BatchNorm`层和`Scale`层两个实现。）并去掉了所有的`Dropout`层。

之所以要加入BN层，是主要为了解决深度网络中的 Internal Covariate Shift问题，通过对每层输入和输出的空间每个维度normalized，令源空间和输出空间数据处于同一个分布下，能够有效的解决反向传播过程中的梯度弥散/爆炸的问题；同时也可以起到一定正则化的效果，降低模型拟合。**该举措令模型的mAP上升了 2.4%。**

关于BN的讲解，这里有一篇文章讲的特别的清澈明了：[知乎专栏：详解深度学习中的Normalization，不只是BN](https://zhuanlan.zhihu.com/p/33173246)

### 2. High Resolution Classifier
目前大部分的目标检测系统在训练的过程中都经历了如下两个步骤：
1. 利用 $$224 \times 224$$ 的 ImageNet 分类数据集来预训练模型；
2. 增加分辨率（如$$448 \times 448$$），利用目标检测数据集来 fine-tune 模型；

论文指出这种模型下，检测模型除了要从分类任务切换到目标检测任务进行学习切换，
同时还要适应分辨率的变化。因此，YOLOv2中将上面的两个步骤，改为三个步骤：
1. 利用 $$224 \times 224$$ 的 ImageNet 分类数据集来预训练模型；
2. 利用 $$448 \times 448$$ 的 ImageNet 分类数据集 fine-tune 模型一遍（一般10个epcho即可。）
3. 增加分辨率（如$$448 \times 448$$），利用目标检测数据集来 fine-tune 模型；

因为 YOLOv2模型是一个 FCN(Full Convolutional Network) 网络，利用GAP(Global Average Pooling)技术来进行信息的整合，因此模型对输入的图片大小尺寸是没有要求的。所以在第一和第二步之间能够无缝的切换。增加了第二步后，模型能够先适应高分辨率的图片，之后再专心于目标检测的任务中。**这一项改进令模型的mAP增加了约4%**

### 3. Convolutional with Anchor Boxes

在 YOLOv1 算法中(详细可以参考上一篇[博文](http://dreamingo.github.io/2018/07/yolov1/))，输入模型的原图先被 Resize 成 $$448 \times 448$$，然后将图片均等的切分成 $$7 \times 7$$个grid，并且在这基础上加了很多空间上的限制(个人感觉整个算法有些生硬)：

1. 对于每个grid， 生成两个 proposal-boxes，并且通过回归来预测这两个box的偏移值和置信度得分。
2. 针对每个 grid 预测其物体类别，并且**限定**每个grid的两个box都只能属于一个类别。

这些限制令 YOLOv1 的recall偏低。受 Faster-RCNN 中 PRN 网络的影响(具体可以看这篇[博文](http://dreamingo.github.io/2018/07/faster-rcnn/))，YOLOv2 算法中也采取从最后一层卷积层的特征图中映射回原图并生成不同尺寸的 anchor-boxes。具体算法细节如下：

1. 在检测模型中，YOLOv2 采取 $$416 \times 416$$ 的图片作为输入。论文中解释这样做的原因是因为YOLO的下采样 $$\{strides = 32\}$$，所以在最终的特征图大小为 $$13 \times 13$$。这样维数是奇数，特征图就只会有一个中心点。
2. 与 Faster-RCNN 中类似，每个特征点映射回原图后，以改点为中心生成 5 个不同尺寸的anchor-boxes。（这5个anchor-boxes的尺寸待会 anchor-dimension cluster 一节中介绍）
3. 针对每个 anchor-box，模型会回归输出25个值（假设是VOC数据集，共20类）。其中包含：这个box的4个坐标偏移量值，物体的置信度得分，以及20个类别的概率；

改成 anchor-boxes 机制后，模型的recall从 81% 上升至 88%（因为YOLOv1中只有98个检测框，而YOLOv2算法则有 $$13 \times 13 \times 5 = 845$$个检测框）。而mAP机制有下幅度的下降(0.4%)。

<figure>
	<a href=""><img src="/assets/images/yolov2/output.png" alt="" width="800" heigh="800"></a>
    <figcaption><a href="" title="">YOLOv2模型的输出</a></figcaption>
</figure>

**在上图中，我们用红框突出了cell这个概念。在YOLOv2中，虽然没有像YOLOv1那样明确将原图划分为 grid-cell 的概念。但是其实在v2中也有 grid-cell 这个概念。由于anchor-box是在最后的特征图上生成的（假设大小为 $$13 \times 13$$），那么可以理解为其将原图划分为 $$13 \times 13$$ 个grid-cell**

### 4. Dimension Cluster

在 Faster-RCNN 中，针对每个映射到原图的点，会生成3个不同尺寸$$\{128^2, 256^2, 512^2\}$$和3个不同长宽比$$\{1:1, 1:2, 2:1\}$$， 共9个anchor-boxes。这些数据都是提前人工指定好，然后再通过 bbox 回归对其大小进行调整。如果这些先验框指定的足够合适，那对模型的学习来说当然是一件好事了，因为省了很多额外调整学习的功夫。

然而针对不同的数据集，可能都需要人工指定不同的先验框大小，这是不太合理的。因此在YOLOv2 算法中，作者抛弃了人工先验指定的套路，而是对训练样本中已有的bounding-box执行kmeans聚类。从已有的数据集中学习出能够代表的先验框。

考虑到用欧几里得距离来衡量聚类时的距离的话，large-box会比small-box贡献更多。因此论文中，box和聚类中心点的距离更改为：

$$
d(box, centroid) = 1 - IoU(box, centriod)
$$

模型从精度和效率上进行这种，最后聚类的数目选择为5。对于两个数据集，5个先验框的width和height如下所示（来源：YOLO源码的[cfg文件](https://link.zhihu.com/?target=https%3A//github.com/pjreddie/darknet/tree/master/cfg)），这些值并非像素值，从代码实现上看，应该是相对于预测的特征图大小（ $$13\times13$$ ）：

```
COCO: (0.57273, 0.677385), (1.87446, 2.06253), (3.33843, 5.47434), (7.88282, 3.52778), (9.77052, 9.16828)
VOC: (1.3221, 1.73145), (3.19275, 4.00944), (5.05587, 8.09892), (9.47112, 4.84053), (11.2364, 10.0071)
```

### 5. Direct Location prediction

这一点讲解起来有些费劲，而且在我看来也不是十足的亮点。因此，这里就偷懒不进行讲述了。详细可以参考引用[[2](https://zhuanlan.zhihu.com/p/25167153?refer=xiaoleimlnote)][[3](https://zhuanlan.zhihu.com/p/35325884)]

总结而言，就是因为以前 bbox 回归时，预测的偏移向量$$\{t_x, t_y, t_w, t_h\}$$的值并没有约束，这导致模型在刚开始训练时候的不稳定性。因此YOLOv2改用了预测边框中心点相对于对应 grid-cell 左上角点的偏移量，并且规定该偏移量不超过1（利用sigmoid函数来限定，也就是说预测框的中心点不超过 grid-cell 范围内）

### 6. Fine-Grained Feature

在YOLOv2检测过程中，输入图片的大小为 $$416 \times 416$$，经过5次 `max-pooling`后，最后的特征图大小为 $$13 \times 13$$。该特征图的感受野足够的大，
对于检测大物体是没有问题的。（关于感受野和偏移量的计算可以参考 [Faster-RCNN 的博文](http://dreamingo.github.io/2018/07/faster-rcnn/)）。YOLOv2 从 SSD 中收到了一些启发，
采取了多尺度的特征图来检测物体，利用前面卷积层中更精的特征图(感受野更小)更有利于用来预测小物体。

YOLOv2中提出一个叫做 `passthrough-layer` 的层，该层共有两个功能：
* 将某一层特征图（在YOLO中前一层特征图大小为 $$26 \times 26 \times 512$$）进行**特征重排**：（$$26 \times 26 \times 512$$ 的feature map分别按行和列隔点采样，可以得到4幅$$13 \times 13 \times 512$$的特征，把这4张特征按channel串联起来，就是最后的$$13 \times 13 \times 2048$$的feature map）

<figure>
	<a href=""><img src="/assets/images/yolov2/pass.png" alt="" width="500" heigh="500"></a>
    <figcaption><a href="" title="">passthrough层的特征重排工作</a></figcaption>
</figure>

* 类似 Residual-Network 的 eltwise 操作，将两层特征图的输出连接（concatenate）起来。最后一起交由 $$1 \times 1$$ 卷积层来做回归操作。

<figure>
	<a href=""><img src="/assets/images/yolov2/pass2.png" alt="" width="600" heigh="300"></a>
    <figcaption><a href="" title="">passthrough层的特征连接工作</a></figcaption>
</figure>

另外，作者在后期的实现中借鉴了ResNet网络，不是直接对高分辨特征图处理，而是增加了一个中间卷积层，先采用64个 $$1\times1$$ 卷积核进行卷积，
然后再进行passthrough处理，这样 $$26\times26\times512$$ 的特征图得到 $$13 \times 13\times 256$$ 的特征图。这算是实现上的一个小细节。
使用Fine-Grained Features之后YOLOv2的性能有1%的提升。

### 7. Multi-Scale Trainning
> 由于YOLOv2模型中只有卷积层和池化层，所以YOLOv2的输入可以不限于 $$416\times416$$ 大小的图片。
为了增强模型的鲁棒性，YOLOv2采用了多尺度输入训练策略，具体来说就是在训练过程中每间隔一定的iterations之后改变模型的输入图片大小。
由于YOLOv2的下采样总步长为32，输入图片大小选择一系列为32倍数的值： $$\{320, 352,..., 608\}$$ ，输入图片最小为 $$320 \times 320$$ ，此时对应的特征图大小为 $$10\times10$$ （不是奇数了，确实有点尴尬），而输入图片最大为 $$608\times608$$ ，对应的特征图大小为 $$19\times19$$ 。在训练过程，每隔10个iterations随机选择一种输入图片大小，然后只需要修改对最后检测层的处理就可以重新训练。-- 引用自参考文章[[3](https://zhuanlan.zhihu.com/p/35325884)]


> 采用Multi-Scale Training策略，YOLOv2可以适应不同大小的图片，并且预测出很好的结果。在测试时，YOLOv2可以采用不同大小的图片作为输入，在VOC 2007数据集上的效果如下图所示。可以看到采用较小分辨率时，YOLOv2的mAP值略低，但是速度更快，而采用高分辨输入时，mAP值更高，但是速度略有下降，对于 $$544\times544$$ ，mAP高达78.6%。注意，这只是测试时输入图片大小不同，而实际上用的是同一个模型（采用Multi-Scale Training训练）。 -- 引用自参考文章[[3](https://zhuanlan.zhihu.com/p/35325884)]

### 8. Faster:Darknet Architecture
YOLOv2区别于传统基于VGG模型的目标检测器，采用了一种称为 Darknet-19 的网络架构。其中包括19个卷积层和5个max-pooling层。总体而言，该网络主要有两个小trick：
1. 去掉全连接层，利用 $$1 \times 1$$ 卷积层 + GAP（Global Average Pooling）的结构来进行分类预测（节省了大量的网络参数）。
2. 在 $$3\times 3$$卷积层前，利用 $$1 \times 1$$对前面特征通道之间的进行信息整合，压缩特征图的channels以减少计算量。

使用了darkent结构后，网络的mAP基本没变，但是计算量却减少了 33%。

<figure>
	<a href=""><img src="/assets/images/yolov2/network.png" alt="" width="400" heigh="800"></a>
    <figcaption><a href="" title="">darknet-19网络架构</a></figcaption>
</figure>

## 0x03. YOLO9000

这部分工作暂时先不写了。。 有空再写吧。最近一口气写了很多文章，累。具体可以参考引用文章。

## 0x04. 引用

1. [1][知乎：如何评价最新的YOLOv3？](https://www.zhihu.com/question/269909535)
2. [2][知乎专栏：YOLO2](https://zhuanlan.zhihu.com/p/25167153?refer=xiaoleimlnote)
3. [3][知乎专栏：目标检测-YOLOv2原理与实现(附YOLOv3)](https://zhuanlan.zhihu.com/p/35325884)
4. [4][Real-time Object Detection with YOLO, YOLOv2 and now YOLOv3](https://medium.com/@jonathan_hui/real-time-object-detection-with-yolo-yolov2-28b1b93e2088)
5. [5][YOLO9000:Better, Faster, Stronger](https://arxiv.org/pdf/1612.08242.pdf)
