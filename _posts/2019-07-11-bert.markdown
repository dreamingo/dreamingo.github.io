---
layout: post
title: "Transformer系列: BERT十问十答"
modified:
categories: 机器学习 
description: "deep-learning transformer bert"
tags: [bert transformer language-model]
comments: true
mathjax: true
share:
date: 2019-07-11T10:49:40+21:12
---


> 本文不对BERT作详细的介绍，而是将阅读过程中的一些问题整理出来，并通过自问自答的形式达到学习和思考的作用；


## 0x01. 前言：

BERT自从去年10月份出来后，一直也没仔细的去研读。截止现在，网上的各种解读、论文导读也百花齐放。因此，本文就不再对BERT作详细的介绍，而是将阅读过程中的一些问题整理出来，并通过自问自答的方式达到学习和思考的作用。

## 0x02. 问题：

### 2.1 BERT中，Bidirentional主要体现在哪里？

语言模型的本质都在回答一个问题：出现的语句是否合理通顺。因此传统的语言模型的定义，都是从左到右根据上文去预测下文的单词。例如：

```
"the man went to a store"
P(the | <s>)*P(man|<s> the)*P(went|<s> the man)*...
```

也许谈到这里，有人跳出来反驳说，Word2vec 结构中（CBOW/Skip-gram），都没有强制要求是从左到右去根据上文预测下文额？那是因为 Word2vec 的目标学习函数根本就不是一个语言模型的训练任务。
其本质想要的 word-embedding 这个任务的副产物，整个任务的设计当然是不一样的。

回到话题本身，从左到右的语言模型，GPT/GPT-2 这类的Auto Regressive 模型正是这么运作的。其利用**Transformer的decoder部分来达到语言模式的效果**。这里我再发散的讲下 decoder 和 encoder 的最大不同：

> 在Transformer中，decoder 和 encoder 最大的不一样，不在于decoder多了一层encoder-decocder attention layers（因为如果没有encoder的话，这一层是相当于不存在的，具体可以参考tensor2tensor的源码实现。）
而是再在于在进行 self-attention 的时候，由于 Mask 的存在，decoder 永远都只能和自己**左边的数据**发生交互。而encoder部分的self-attention，则是和**全部位置**的信息发生任意的交互。

但是可惜的是，这种decoder的做法，只能看到前向上文的信息。如果我们想要学习到每个单词最好的 contexual representation的话，这会导致很多信息的丢失。

而更自然的一种做法，便是同时训练一个逆序模型，根据后文去预测前文，然后将两个模型得到的 representation 连接起来。这就是ELMo中『双向』的表示。但是，这种做法本质上还是通过两个模型来进行，总体的loss
是通过两个子模型相加得到。既不自然，也不优雅，分离的模型会带来更多的信息折损。

<figure>
	<a href=""><img src="/assets/images/bert/bid.jpg" alt="" width="700" heigh="400"></a>
    <figcaption><a href="" title="">BERT 和 GPT、ELMo在双向结构上的异同</a></figcaption>
</figure>


而BERT采取一种更加自然的『双向』编码：利用 **Transformer-encoder** 来完成语言模型中双向进化。在上面中我们提到，encoder结构在做 self-attention 的时候，当前位置能够和任意的位置发生交互，实现了『双向』的概念。
在这之前，之所以不能通过这种方式来做语言模型，主要是因为这种『双向』的方式，令当前的位置在高层的信息中，间接能够『偷窥』到要预测单词的信息，这对于一个 Text2Self 的任务来说，这样的设置会令整个语言模型的训练变得毫无意义
（例如很可能在做self-attetion的时候，自身位置信息编码的向量权重永远为1，其他向量权重为0.）

那么，BERT 在利用 encoder 做双向的同时，是怎么解决这个问题的呢？请看第二个问题。


### 2.2 BERT中，为什么需要引入 Mask 这一个任务？

在上面的第一个问题，我们谈到说双向的结构会在语言模型的这个任务中，在预测当前单词的时候，会令数据窥探到『自己』，也就是要预测的内容。这样的设置会令这个语言模型的训练任务变得没有意义。根据
BERT的一作 Jacob 在 [Reddit](https://www.reddit.com/r/MachineLearning/comments/9nfqxz/r_bert_pretraining_of_deep_bidirectional/) 上的原话：

> It's unfortunately impossible to train a deep bidirectional model like a normal LM, because that would create cycles where words can indirectly "see themselves," and the predictions become trivial.


那这个所谓的数据窥探，具体是怎么造成的呢？在这里，我画了一个不那么『严谨』的关系图：在作全局的 self-attention的时候，自由的联通会使得**两层网络堆叠之后，B就能从下层的其他地方『窥探』到自己了。**

<figure>
	<a href=""><img src="/assets/images/bert/see.jpg" alt="" width="300" heigh="300"></a>
    <figcaption><a href="" title="">双向self-attention导致的『偷窥』问题</a></figcaption>
</figure>

因此，在BERT中，通过如下加mask的策略，令模型完成一个 Mask LM（Cloze task）来解决这个问题：**原句中 15% 的token 可能以[MASK] 标记代替，而在其中15%，80%真正的加Mask，10%保持不变，而10%则随机替换其他词。** 并且，模型的softmax层**仅仅预测 Mask的单词**
（也就是说只有Mask位置的单词才加入到loss计算中去），这样通过完全将预测的单词信息从源头抹去，任凭上下文来对其进行预测。顺利的解决了数据『窥视』的这个问题。同时，在论文中作者也提到，由于每一个语句中仅仅有15%的单词加入到真正的模型训练中（对loss有真正贡献），因此迭代起来会比较慢，需要更多的迭代步数和时间。 

在上面一段的策略中有提到 『其中15%，80%真正的加Mask，10%保持不变，而10%则随机替换其他词』，为什么要这么做呢？ 请看下一个问题。

### 2.3 BERT中，Mask为什么要如此设置？

BERT对原句的Mask策略中有提到： 『其中15%，80%真正的加Mask，10%保持不变，而10%则随机替换其他词』，为什么要这么做呢？ 原文中是这么解释的：

> Although this allows us to obtain a bidirectional pre-trained model, a downside is that we are creating a **mismatch** between pre-training and
fine-tuning, since the [MASK] token does not appear during fine-tuning.

重点在于 mismatch 上，那么到底是什么意思呢？我在知乎上找到了一个很好的[回答](https://www.zhihu.com/question/326704953/answer/698621923)，这里别人珠玉在前，我也不多献丑了，直接引用过来：

```python
谢邀。 

以前读到过一个概念，叫“Exposure Bias”（之前写错了，已修正）。说的是，RNN 在训练时使用 ground-truch 喂给 decoder，预测却只能使用上一个时序的生成结果。即，训练模型时的数据使用方式，和生产环境的实际情况存在偏差。 
放到 BERT 里也是一样。pretrain 时预测的词被 [MASK] 住，即得到的是“完全基于上下文“的深度语义向量。finetune 时，再也没有 [MASK] 这个标签了，并且当前词是可见的，它会并且应当参与到最终的语义生成。

这里面就有“bias”。 

所以呢，留着80%的词 [MASK]，10%的词保留。剩下10%的词随机替换，我认为可以理解为“加噪”，告诉模型，输入的词有可能是错误的，不要太相信它。毕竟训练数据不是100%干净的。 

我没有做过对比实验，但是作者这样设置必定是反复评估过效果的。所以经典文章细致地读很有必要，那些 trick 一定不是拍脑袋想出来的。

# 原文作者：小莲子
# 原文链接：https://www.zhihu.com/question/326704953/answer/698621923
# 原文出处：知乎
```



### 2.4 BERT中，为什么需要引入 Next Sentences Prediction 这一任务？

这个问题相比之下比较容易回答，由于类似QA问答、NLI 自然语言推理等任务，往往是建立在理解两句话的关系的基础上的。除了上面的 MLM 任务之外，BERT 在 pretrain 阶段
引入了新的一个 Next-Sentence Predicrtion(NSP)任务。在一同输入的sequence序列中，往往包含着A,B两个句子。其中训练任务中，50%的B是真实语料中A句子的下一句，另外50%则是随机采样的句子。
通过这样构造一个简单有效的二分类任务（利用BERT最终输出序列中的`<CLS>` token 向量接softmax层进行分类），因为这个任务的存在，作者也强调在训练语料中，最好是使用『document level』的有序语料。
避免使用shuffled过的 sentence-level的语料。

<figure>
	<a href=""><img src="/assets/images/bert/nsp.jpg" alt="" width="800" heigh="400"></a>
    <figcaption><a href="" title="">BERT输入中，两个pack在一起的句子，通过[sep]符号分割，并辅以segment embedding进行区分</a></figcaption>
</figure>

个人觉得，这个任务的引入，除了正如论文所说的『有助于理解句子间的关系外』，由于 self-attention 这种无视空间距离的神器存在，这种两句拼接、更长的句子，也使得句子间的单词能够更充分从片段上下文直接学习到更加充分和solid的repreentation


### 2.5 BERT中，Positional-embedding 为什么应该这样设置？

在BERT中，Positional-embedding 是通过在训练中学习出来的。这一点和论文 《[Attention is All You Need](https://arxiv.org/abs/1706.03762)》中『人工定义』的三角函数的position-embedding有所不一样。究其原因，个人主要想到了两点：

* 在论文《Attention is All you Need》中，由于论文任务是翻译任务，受限于平行语料的数量，作者只出 predefine 的模式和学习得到的 pos-embedding 模式并没有太大的区别。那优先肯定使用更加简单的设置；
* 而在BERT中，论文任务是语言模型，拥有海量的训练数据，个人猜测在这种情况下，学习得到的 embedding 信息更为丰富和有效。

<figure>
	<a href=""><img src="/assets/images/bert/pe.jpg" alt="" width="800" heigh="400"></a>
    <figcaption><a href="" title="">BERT输入中，两个pack在一起的句子，通过[sep]符号分割，并辅以segment embedding进行区分</a></figcaption>
</figure>


### 2.6 BERT中，输入的`[CLS]` token的向量，为什么能够直接作为分类任务输入？

在BERT中，下游的分类任务，都可以直接使用 [CLS] token 作为下游的分类任务。这种的做法初看下来，好像有些新颖，也有些迷惑。仔细分析下来，惊叹设计的巧妙之处。

回顾下在以往利用神经网络来做文本分类任务的情景，以 fasttext 为例，其将多个 token 的embedding 信息直接均化融合；以 textCNN 为例，则是将多个 extract-featuure 进行pooling处理，得到融合向量。
那么核心的观点都无非只有一个：**对下游不定长的特征进行信息聚合，供上游任务使用。**

相比于传统粗暴的信息聚合方式（pooling（max/average等）），Self-Attention 在这方面有得天独厚的优势！回想一下，**`[CLS]` token的最终输出表示，不正是由下层所有token的特征，利用Self-attention加权聚合而来的么**。
并且这种端到端的学习，比起传统的pooling技术，更加的自然优雅，对数据的刻画能力自然也是更上一层楼了。

至于为啥 [CLS] 要放在句子开头，这个问题就更加容易回答了。放在句子中间，影响文本通顺信息；放在句子末尾，理论上也行，不过可能会有 <EOS> 符号的影响。

<figure>
	<a href=""><img src="/assets/images/bert/cl.jpg" alt="" width="800" heigh="400"></a>
    <figcaption><a href="" title="">BERT中直接利用[CLS] token 作为下游分类任务的输入</a></figcaption>
</figure>

## 0x03. 小结

其实和GPT相比，BERT不是严格的语言模型，因为其并不能做生成类任务。但是这点也和 word2vec 中有些相似，BERT『创新』的将自己跳脱在语言模型的束缚之外，通过各种技巧设计整体模型的损失函数，立足点在于服务下游的任务。
因此才有了横扫N项下游任务的荣誉，不得不惊叹于佩服其设计之妙。

另外标题中不是说好是十问十答的吗！怎么只有6个问题！ 没有力气写了，好一些小问题，也直接融合到上面中一起回答去了。。至于BERT的作用和丰功伟绩，知乎上大佬评价丰富多彩，这里小弟也就不班门弄斧了。

## 0x04. 引用

* [1][BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)
* [2][知乎：如何评价 BERT 模型？](https://www.zhihu.com/question/298203515)
* [3][[知乎专栏：穆文：NLP - 从语言模型看Bert的善变与GPT的坚守](https://zhuanlan.zhihu.com/p/66409688)
