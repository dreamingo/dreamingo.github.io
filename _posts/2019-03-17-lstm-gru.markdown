---
layout: post
title: "RNN系列：LSTM & GRU &源码剖析"
modified:
categories: 机器学习 
description: "deep-learning rnn source-code"
tags: [deep-learning rnn source-code lstm]
comments: true
mathjax: true
share:
date: 2019-03-07T12:23:12+08:00
---

> 本文是 RNN 系列的第二篇文章，旨在介绍为什么需要LSTM&GRU，以及从实现源码的角度上对两者进行剖析。
> 本文主要是在[colah blog: Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)的基础上进行总结和升华，建议阅读本文前，先认真读完上面的引用文章。


## 0x01. 先抛问题

在上一篇的文章中，我们简要的介绍了RNN的基础原理和实现细节；那么在本章中，我们即将介绍的两位主角是LSTM 和GRU。对于两者，我们先抛出一系列的问题：

1. 为什么提出了LSTM结构，其改进了RNN中的哪些问题？
2. 为什么提出了GRU结构，和LSTM对比主要优点体现在何处？
3. 两者在TensorFlow实现细节上，具体是如何完成的？

在接下来的章节中，我们围绕着这三个问题进行讨论。

## 0x02. RNN问题在哪？

在上一篇文章中，我们介绍了RNN经典的结构。在下图中我们可以看到，RNN内部时通过不断的维护一个隐含状态$$h_t$$来对历史序列信息进行建模的。其中隐含状态$$h_t$$总是通过不断**递归更新**：$$h_t = f(h_{t-1}, x_t)$$。

<figure>
	<a href=""><img src="/assets/images/rnn/rnn.jpg" alt="" width="700" heigh="300"></a>
    <figcaption><a href="" title="">RNN基础结构图, 出自[colah's blogs](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)</a></figcaption>
</figure>

其实问题就出在**递归更新**这里。根据导数的链式法则我们可以知道，这种递归式的函数求导形态，最后都是变成了**连乘**的形式。例如：

$$
\begin{equation}
\begin{aligned}
\frac{\partial{L}}{\partial{h_t}} = \frac{\partial{L}}{\partial{h_{t+n}}} \frac{\partial{h_{t+n}}}{\partial{h_{t+n-1}}}...\frac{\partial{h_{t+1}}}{\partial{h_{t}}}
\end{aligned}
\end{equation}
$$

_因此，RNN最大的问题在这种梯度连乘的形态，在长距离捕捉特征时所面临的**梯度消失**的问题（梯度爆炸并非严重的问题，通过梯度裁剪后优化算法可以解决）。


## 0x03. LSTM&GRU

> 本文并不打算非常详细的介绍LSTM和GRU，建议阅读引用文章[colah's blogs](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)，珠玉在前，我这里只做归纳性总结。但是为了方便后面文章开展，在本小节中对一些基础概念和符号进行复述。

### 3.1 符号定义
在开始之前，我们对之后会出现的图标中的符号作统一展示：
<figure>
	<a href=""><img src="/assets/images/lstm_gru/symbol.jpg" alt="" width="600" heigh="300"></a>
    <figcaption><a href="" title="">符号定义, 出自[colah's blogs](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)</a></figcaption>
</figure>

除了操作性符号，这里还对结构内部的数据作统一说明：

* $$h_t$$: $$t$$ 时刻，LSTM输出的隐含状态向量(hidden-state)
* $$x_t$$: $$t$$ 时刻，LSTM的输入数据；
* $$C_t$$: $$t$$ 时刻，LSTM中CellState的输出。

### 3.2 门(Gate)的定义

在接下来的讲述中，会经常提到**“门(Gate)”**的这个概念。在这里，可以将其表示为一个开关。我们通常用一个“门”来控制（element-wise product）**信息的通过与否**。如果门为0，则信息完全不通关。1则表示完全通过。因为单纯的 0/1 门在数学中无法求导。因此利用 `sigmoid` 函数来代替。 Sigmoid 函数的输出大小表示**通过的比率**。

在实现中，“门”是通过 `Sigmoid` 函数 + `element-wise` 乘法实现的。例如 $z = x \times y$, 我们利用 $y$ 来控制  $x$ 对最终结果 $z$ 的影响。表示通过一个变量来控制另外一个变量成分通过。

<figure>
	<a href=""><img src="/assets/images/lstm_gru/gate.jpg" alt="" width="600" heigh="300"></a>
    <figcaption><a href="" title="">门Gate的定义</a></figcaption>
</figure>


### 3.3 LSTM(Long-Short-Term-Memory network)

和传统的RNN结构相比，LSTM同样也是chain-like的结构。
可以看到，LSTM内部数据交互结构却大有不同，同时除了和传统RNN一样拥有hidden-state $$h_t$$ 和 输入 $$x_t$$ 之外，还多了一个 CellState $$C_t$$。

<figure>
	<a href=""><img src="/assets/images/lstm_gru/lstm.jpg" alt="" width="800" heigh="400"></a>
    <figcaption><a href="" title="">LSTMa与传统RNN结果对比图</a></figcaption>
</figure>

#### 3.3.1 **Cell-State**
<!-- <span style="color:#AA0000"><strong>在LSTM中，最关键的就是CellState的引进。这个改进直接的使得LSTM有效的避免了RNN梯度消失的问题。</strong></span> -->
> 在LSTM中，最关键的就是CellState的引进。这个改进直接的使得LSTM有效的避免了RNN梯度消失的问题。

LSTM结构图最上方，一条直直的水平线正是所说的Cell-State。其类似于一条长长的信息传输带，保证了信息在不同时刻LSTM结构内部的流通，其主要功能有：
* 记录历史时刻的信息，保证了历史信息的有效传递；
* 结合当前时刻的输入，对传输带Cell-State的数据进行选择性**遗忘**和**更新**；
* 更新完毕后，将数据流入到下一状态。

<figure>
	<a href=""><img src="/assets/images/lstm_gru/cellstate.jpg" alt="" width="600" heigh="300"></a>
    <figcaption><a href="" title="">LSTM - CellState结构</a></figcaption>
</figure>


#### 3.3.2 **信息筛选：遗忘门（Forget-Gate）**

在LSTM中，CellState作为信息传输带，带来了历史状态的信息总结。
在 Forget-Gate 在此处的作用就是：
根据当前时刻的输入$$x_t$$和隐含状态$$h_{t-1}$$，来决定保留CellState的哪些信息。

<figure>
	<a href=""><img src="/assets/images/lstm_gru/forget_gate.jpg" alt="" width="600" heigh="300"></a>
    <figcaption><a href="" title="">LSTM - Forget-Gate结构</a></figcaption>
</figure>

在上图中我们可以看到，Forget-Gated正是利用了之前提到**门（Gate）**的方式来运作的：
* 将当前输入$$x_t$$和隐含状态$$h_{t-1}$$ 连接(concat)起来，作为当前LSTM信息的输入；
* 将上述结果经过Sigmoid函数，生成遗忘信息表；
* 将遗忘信息表和CellState结果进行element-wise 乘法，决定CellState矩阵中每个信息元素的通过程度；_

在引用文章中还举了一个生动的例子：当我们遇到一个主语时，会记住这个主语的信息，以便在预测相关动词时利用该信息。但是当我们遇到下一个新的主语，那么CellState中应该将上一个主语的信息进行选择性遗忘。


#### 3.3.3 **信息补充**

在上一步中，我们通过遗忘门对信息进行筛选后，那么接下来的操作，应该就是根据当前的输入情况，对CellState的信息进行补充；

<figure>
	<a href=""><img src="/assets/images/lstm_gru/input_gate.jpg" alt="" width="600" heigh="300"></a>
    <figcaption><a href="" title="">LSTM - Input-Gate结构</a></figcaption>
</figure>

在上图中，我们通过：
* 将当前输入$$x_t$$和隐含状态$$h_{t-1}$$ 连接(concat)起来，作为当前LSTM信息的输入；
* 计算得到向量$$\tilde{C_t}$$，作为本次**输入的信息总结**；
* 计算得到向量$$i_t$$，其作用主要是作为门Gate，控制哪些提炼物应该补充到CellState中；
* 将向量$$i_t$$和$$\tilde{C_t}$$进行 element-wise 乘法，**提炼**出本次输入中哪些信息应该整合到CellState中；

下图正式通过 element-wise 加的方式，将信息整合到CellState中。正是因为element-wise加的存在。使得LSTM有效的避免的梯度消失（后面章节详细分析）

<figure>
	<a href=""><img src="/assets/images/lstm_gru/add.jpg" alt="" width="600" heigh="300"></a>
    <figcaption><a href="" title="">LSTM - 信息整合</a></figcaption>
</figure>


#### 3.3.3 **信息输出**

根据当前输入，对历史的信息进行过滤，补充之后，那么顺其自然的下一步就应该到了信息输出了。

<figure>
	<a href=""><img src="/assets/images/lstm_gru/output.jpg" alt="" width="600" heigh="300"></a>
    <figcaption><a href="" title="">LSTM - 信息输出</a></figcaption>
</figure>

在上图中，可以看到输出的信息主体是更新后的CellState: $$C_t$$，但是一个过滤的版本。同样是通过一个Gate(**Output Gate**)来控制CellState，哪些信息应该被输出。


### 3.4 GRU(Gated Recurrent Unit)

在上一小节学习LSTM的时候，不知道大家有没有感觉到一丝的异样：

1. **CellState $$C_t$$和 Hidden-State $$h_t$$ 在功能上是否有些重复呢？**，这两者的作用都是起到对历史信息的承上启下。那么是否可以合并两者呢？
2. **决定遗忘哪些信息和决定更新哪些信息，是否有互斥的关系？**，如果是，是否不需要两个子神经网络来学习遗忘和补充关系？

是的，GRU的提出正是解决了上面的两个**异样**。众多论文表示，GRU和LSTM在很多任务上性能相当，但是GRU总体计算量更少。（再后续章节，我们会详细的分析下两者的计算量差异）

<figure>
	<a href=""><img src="/assets/images/lstm_gru/gru.jpg" alt="" width="600" heigh="300"></a>
    <figcaption><a href="" title="">GRU结构小窥</a></figcaption>
</figure>

从上图中可以看到，GRU和LSTM最主要的不同在于：

1. GRU将CellState和HiddenState合并了，统一为$$h_t$$
2. GRU将forget-gate和input-gate合并了，统一变成了update-gate（可以看到，1 - forgetgate 的结果，正是原来input-gate的参数）


## 0x04. 解决问题：

讲解完基础原理后，我们回过头来重新思考最开始抛出的几个问题。

### 4.1 LSTM 是如何解决 RNN 梯度消失的问题？
在前面我们提到，RNN中出现梯度消失的问题，主要来源于反向传播时梯度连乘的问题；同时，我们在介绍LSTM的时候，也提到过，
**CellState的引入是LSTM解决梯度消失的问题根本所在**。那么，具体应该怎么理解这句话呢？

受到本博客之前博文[Residual Network详解](http://dreamingo.github.io/2018/02/residual_net/)的影响。一开始我以为CellState类似于Residual-Network中的shortcut连接，其存在使得不同层/不同状态时候的
信息能够顺畅的流通。

<figure>
	<a href=""><img src="/assets/images/lstm_gru/residual_network.jpg" alt="" width="700" heigh="700"></a>
    <figcaption><a href="" title="">ResidualNetwork 一文中，shortcut的identity-mapping的形式，使梯度信息能够顺利流通</a></figcaption>
</figure>

后来在之后知乎上，也看到有类似的[回答](https://www.zhihu.com/question/34878706/answer/70422467)，指出现代的LSTM模型，其隐含状态往往是通过累加的形式得出的$$S_t = \sum_{i=1}^{t}\Delta{S_i}$$的方式得出。

然而，在实际深入了解之后，发现CellState的更新中，**还是会受到forget-gate中element-wise-product的影响。**如此仔细一想，上面中所提到的累加模型(或者类似Residual-Network中提到的identity-mapping)就不存在了啊~。
反向传播时多次的forget-gate乘法还是会导致梯度消失？

<figure>
	<a href=""><img src="/assets/images/lstm_gru/add.jpg" alt="" width="600" heigh="300"></a>
    <figcaption><a href="" title="">LSTM - CellState的更新公式</a></figcaption>
</figure>

几经折腾，后来在知乎一个回答下面中的讨论，看到了[Tower大神](https://www.zhihu.com/people/SeptEnds/activities)对此的一些解释，才有了醍醐灌顶的感觉，这里直接引用下该讨论。

> 1. 原始的 LSTM 是没有 forget gate 的，或者说相当于 forget gate 恒为 1，所有不存在梯度消失问题(那么identity-mapping的等式就成立了)；
> 2. 现在的 LSTM 被引入了 forget gate，但是 **LSTM 的一个初始化技巧就是将 forget gate 的 bias 置为正数（例如 1 或者 5，这点可以查看各大框架源码）**，这样一来模型刚开始训练时 forget gate 的值都接近 1，不会发生梯度消失；(可以参考引用[4]的论文，本质上是forget-gate中的参数初始化时为均值为0的正态分布(大部分参数为0.x)，连乘后会导致梯度弥散，加上1之后，参数不再小于1，一定程度上简单的避免了梯度弥散)
> 3. 随着训练过程的进行，forget gate 就不再恒为 1 了。不过，一个训好的模型里各个 gate 值往往不是在 [0, 1] 这个区间里，而是要么 0 要么 1，很少有类似 0.5 这样的中间值，其实相当于一个二元的开关。假如在某个序列里，forget gate 全是 1，那么梯度不会消失；否则，若某一个 forget gate 是 0，这时候虽然会导致梯度消失，但这是 feature 不是 bug，体现了模型的选择性（有些任务里是需要选择性的，比如情感分析里”这部电影很好看，但是票价有点儿贵“，读到”但是“的时候就应该忘掉前半句的内容，模型不想让梯度流回去）；
> 3. 基于第 3 点，我不喜欢从梯度消失/爆炸的角度来谈论 LSTM/GRU 等现代门控 RNN 单元，更喜欢从选择性的角度来解释，模型选择记住（或遗忘）它想要记住（或遗忘）的部分，从而更有效地利用其隐层单元。

总结而言，LSTM中尽管CellState的更新公式不满足完全的identity-mapping的累加形式，但是通过一些trick的方法（上面提到的bias初始化），以及加法门的引入，还是在很大程度上缓解了RNN的梯度消失的问题。



### 4.2 GRU 和 LSTM 相比，计算量少在哪？

啊，博文写到这里就很累了， 这里就贴出两张图，看客根据矩阵大小的对齐规则， 简单的推导下吧：

<figure>
	<a href=""><img src="/assets/images/lstm_gru/lstm_gru.jpg" alt="" width="800" heigh="500"></a>
    <figcaption><a href="" title="">LSTM vs GRU 计算量比较</a></figcaption>
</figure>


## 0x05. 源码细节

> 源码参照 tensorflow2.0 中的代码，路径为 `tensorflow/python/keras/layers/recurrent.py`

### LSMTCell源码

我们先来看看`LSTMCell`的参数构造是如何进行的。在上面的LSTM运算图中，我们可以看到几个门Gate主要的运算矩阵分别有：

* $$W_f$$: forget-gate 的参数矩阵，矩阵大小为 $$W_f \in R^{units \times (units + input\_dim)}$$
* $$W_i$$: input-gate 的参数矩阵，矩阵大小为 $$W_i \in R^{units \times (units + input\_dim)}$$
* $$W_C$$: 用于转换CellState的参数矩阵，矩阵大小为 $$W_C \in R^{units \times (units + input\_dim)}$$
* $$W_o$$: output-gate，矩阵大小为 $$W_o \in R^{units \times (units + input\_dim)}$$

如果按照之前RNN代码的思路，把所有的矩阵连接合并为一个大的矩阵，则该大矩阵的形状应该为 $$(4 \times units) \times (units + input\_dim)$$

然而，在`LSTMCell`的源码中，代码把这个大的矩阵一分为二，分别变成了：

* `kernel`矩阵: 主要用于和input做矩阵运算，其形状为$$input\_dim \times (units \times 4)$$;
* `recurrent_kernel`矩阵: 主要用于和hiddent-state$$h_t$$做矩阵运算，其形状为$$units \times (units \times 4)$$;

这样分开来实现的主要原因，是可以单独的控制两个矩阵的行为，针对input和hiddent-state的矩阵运算做不同的优化。例如初始化方式、正则化方式、使用的激活函数等(PS: 对于`recurrent_kernel`而言，默认使用的是`hard_sigmoid`激活函数（一种快速近似的sigmoid计算方式，类似于fasttext中使用的分段近似计算），而`kernel`默认使用的则是`tanh`激活函数)。

{% highlight python %}
  @tf_utils.shape_type_conversion
  def build(self, input_shape):
    input_dim = input_shape[-1]
    # 上面段落中提到的 kernel matrix；
    self.kernel = self.add_weight(
        shape=(input_dim, self.units * 4),
        name='kernel',
        initializer=self.kernel_initializer,
        regularizer=self.kernel_regularizer,
        constraint=self.kernel_constraint)
    # 上面段落中提到的 recurrent_kernel matrix；
    self.recurrent_kernel = self.add_weight(
        shape=(self.units, self.units * 4),
        name='recurrent_kernel',
        initializer=self.recurrent_initializer,
        regularizer=self.recurrent_regularizer,
        constraint=self.recurrent_constraint)

    if self.use_bias:
      # unit_forget_bias值为True的话，则表示将bias中，forget-gate中的值全部初始化为1
      # 这样做的目的有助于防止LSTM训练初期的梯度消失,
      # 具体可以参考论文[Jozefowicz et
      # al.](http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)

      if self.unit_forget_bias:

        def bias_initializer(_, *args, **kwargs):
          # bias为forget、input、matrix-cell, output gate四次矩阵运算的bias。其中
          # bias[self.units: self.units * 2]此段为forget-gate的bias
          return K.concatenate([
              self.bias_initializer((self.units,), *args, **kwargs),
              initializers.Ones()((self.units,), *args, **kwargs),
              self.bias_initializer((self.units * 2,), *args, **kwargs),
          ])
      else:
        bias_initializer = self.bias_initializer
      self.bias = self.add_weight(
          shape=(self.units * 4,),
          name='bias',
          initializer=bias_initializer,
          regularizer=self.bias_regularizer,
          constraint=self.bias_constraint)
    else:
      self.bias = None
    self.built = True
{% endhighlight %}

接下看我们来看看`LSTMCell`的Call过程。Call接口的一次调用，是计算一次time-step的过程。其中函数的输入为：
* `inputs`: 2D矩阵，输入大小为 $$batch\_size \times input\_dim$$;
* `states`: List, 上一步timestep的输出，分别是[hiddent-state, cell-state]

函数输出为：
* `output`: 在RNN系列中，输出的output其实都等等于hidden-state。大小为$$batch\_size \times units$$
* `states`: List, 这一步timestep的输出，分别是[hiddent-state, cell-state]

{% highlight python %}
  def call(self, inputs, states, training=None):
    h_tm1 = states[0]  # previous memory state
    c_tm1 = states[1]  # previous carry state

    dp_mask = self.get_dropout_mask_for_cell(inputs, training, count=4)
    rec_dp_mask = self.get_recurrent_dropout_mask_for_cell(
        h_tm1, training, count=4)

    # implement = 1时，会使用分散的小矩阵计算，这样会计算起来慢，但是memory
    # 方面可能更为宽松一些
    if self.implementation == 1:
      # 处理input部分
      if 0 < self.dropout < 1.:
        inputs_i = inputs * dp_mask[0]
        inputs_f = inputs * dp_mask[1]
        inputs_c = inputs * dp_mask[2]
        inputs_o = inputs * dp_mask[3]
      else:
        inputs_i = inputs
        inputs_f = inputs
        inputs_c = inputs
        inputs_o = inputs
      # NOTE: 分别计算：
      # x_i: input-gate
      # x_f: forget-gate
      # x_c: cell-state-gate
      # x_o: output-gate
      x_i = K.dot(inputs_i, self.kernel[:, :self.units])
      x_f = K.dot(inputs_f, self.kernel[:, self.units:self.units * 2])
      x_c = K.dot(inputs_c, self.kernel[:, self.units * 2:self.units * 3])
      x_o = K.dot(inputs_o, self.kernel[:, self.units * 3:])
      # 添加bias
      if self.use_bias:
        x_i = K.bias_add(x_i, self.bias[:self.units])
        x_f = K.bias_add(x_f, self.bias[self.units:self.units * 2])
        x_c = K.bias_add(x_c, self.bias[self.units * 2:self.units * 3])
        x_o = K.bias_add(x_o, self.bias[self.units * 3:])

      # 处理hiddent-state
      if 0 < self.recurrent_dropout < 1.:
        h_tm1_i = h_tm1 * rec_dp_mask[0]
        h_tm1_f = h_tm1 * rec_dp_mask[1]
        h_tm1_c = h_tm1 * rec_dp_mask[2]
        h_tm1_o = h_tm1 * rec_dp_mask[3]
      else:
        h_tm1_i = h_tm1
        h_tm1_f = h_tm1
        h_tm1_c = h_tm1
        h_tm1_o = h_tm1
      x = (x_i, x_f, x_c, x_o)
      h_tm1 = (h_tm1_i, h_tm1_f, h_tm1_c, h_tm1_o)
      # 计算CellState和output-hiddent-state
      c, o = self._compute_carry_and_output(x, h_tm1, c_tm1)
    # implement = 2时，会使用batch的大矩阵计算，这样会计算起来更快，但是memory
    # 方面可能更为紧张一些
    else:
      if 0. < self.dropout < 1.:
        inputs *= dp_mask[0]
      z = K.dot(inputs, self.kernel)
      if 0. < self.recurrent_dropout < 1.:
        h_tm1 *= rec_dp_mask[0]
      z += K.dot(h_tm1, self.recurrent_kernel)
      if self.use_bias:
        z = K.bias_add(z, self.bias)

      z0 = z[:, :self.units]
      z1 = z[:, self.units:2 * self.units]
      z2 = z[:, 2 * self.units:3 * self.units]
      z3 = z[:, 3 * self.units:]

      z = (z0, z1, z2, z3)
      c, o = self._compute_carry_and_output_fused(z, c_tm1)

    h = o * self.activation(c)
    return h, [h, c]

  def _compute_carry_and_output(self, x, h_tm1, c_tm1):
    """Computes carry and output using split kernels."""
    # 计算CellState和hidden-state（output）
    x_i, x_f, x_c, x_o = x
    h_tm1_i, h_tm1_f, h_tm1_c, h_tm1_o = h_tm1
    i = self.recurrent_activation(
        x_i + K.dot(h_tm1_i, self.recurrent_kernel[:, :self.units]))
    f = self.recurrent_activation(x_f + K.dot(
        h_tm1_f, self.recurrent_kernel[:, self.units:self.units * 2]))
    c = f * c_tm1 + i * self.activation(x_c + K.dot(
        h_tm1_c, self.recurrent_kernel[:, self.units * 2:self.units * 3]))
    o = self.recurrent_activation(
        x_o + K.dot(h_tm1_o, self.recurrent_kernel[:, self.units * 3:]))
    return c, o

{% endhighlight %}

剩下关于`GRUCell`的源码实现，本质上两者时差不多的。这是Cell内部的参数不一样和参数的计算方式有些区别，建议读者自己详细读一下源码。


## 0x06. 引用
1. [1].[colah's blogs](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)
2. [2].[LSTM如何来避免梯度弥散和梯度爆炸？](https://www.zhihu.com/question/34878706)
3. [3].[Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling](https://arxiv.org/pdf/1412.3555.pdf)
4. [4].[An Empirical Exploration of Recurrent Network Architectures](http://proceedings.mlr.press/v37/jozefowicz15.pdf)

