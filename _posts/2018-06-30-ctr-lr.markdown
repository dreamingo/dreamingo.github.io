---
layout: post
title: "CTR之路-基础篇：LR详解 & 非线性 & 并行化"
modified:
categories: 机器学习 
description: "CTR学习之路：Logistic Regression"
tags: [CTR lr]
comments: true
mathjax: true
share:
date: 2018-06-30T13:54:50+08:00
---

最近系统的学习CTR预估，发现对LR的一些背后数理，工业应用基本还是不太理解。因此在本篇博文中，我企图是从 LR 的数学原理的角度去切入，辅以工业届中实际的应用情况为例。尽量避免变成一篇小白的 LR 介绍文。

## 0x01. 模型介绍

Logistic Regression, 在英文术语中准确而简洁。虽然 Regression 一词有些歧义，但是这却是一种常见的**分类**算法。在中文翻译有多中译法，例如逻辑回归，对数几率回归。
在本文，我们采用**『对数几率回归』**的这个译法，因为该名称才是这个模型背后的含义所在。

LR 本质也是广义线性模型(generailized linear model)中的一种。在分类问题中，我们需要输出正例的概率$$p$$，因此常常利用 Sigmoid 函数（又称对数几率函数）将模型的输出
转化为 0 - 1 之间的概率值

$$
\begin{equation}
\begin{aligned}
y &= \frac{1}{1 + e^{-z}} \\
  &= \frac{1}{1 + e^{-(\mathbf{w}^T\mathbf{x}+b)}}
\end{aligned}
\tag{1}\label{eq:1}
\end{equation}
$$

我们将上面公式$$\eqref{eq:1}$$中做一下处理，便可以得出如下的公式：

$$
\begin{equation}
ln\frac{y}{1-y} = \mathbf{w}^T\mathbf{x} + b
\tag{2}\label{eq:2}
\end{equation}
$$

其中可以看到$$\frac{y}{1-y}$$我们称之为『几率』(odds),取对数也就是『对数几率』，因为，LR 翻译为『对数几率回归』是我认为比较合适的。
在这里，我们可以看到 LR 使用的**线性回归模型** ($$w^Tx + b$$) 来逼近真实label的对数几率。当然，在别的一些应用场景，我们会发现有别的模型（例如GBDT中用$$F(x) = \sum_{i}^{m}f_i(x)$$来逼近真实label的对数几率。）


## 0x02. 损失函数-对数似然

在进入之前，我们先对似然函数(Likelihood Function)和最大似然估计(Maximun Likelihood Estimation)有一些直观上的了解。

### 似然函数的意义

**似然函数**：函数$$L(B\vert{A})$$表示已知事件A的发生，对模型参数B的估计。

> 在数理统计中，似然函数是一种关于统计模型参数的函数，表示**模型参数的似然性。**其中，『似然性』和『概率』是有明显的区别的。
概率是指在已知模型参数、结果未知的情况下，预测接下来的观测结果。而似然则是利用已有的观测结果，反过来对模型参数进行推断。

在实际的应用中，我们其实根本**不关心似然函数具体的值，只是关心当参数变化是，该值是变大还是变小**。对于同一个似然函数，如果参数$$\theta$$令其值更大。
我们可以认为这个参数更加合理。因为**这个参数令已观测到的结果出现的概率更大**

### 最大似然估计

> 在统计学中，最大似然估计估计一个概率模型参数的方法

给定一个概率分布$$D$$，以及其概率密度函数 PDF（连续）或者概率质量函数 PMF(离散)为$$f_D$$，以及一个分布参数$$\theta$$.我们可以从这个分布中抽出出n个值的采样，$$X_1,X_2, ... X_n$$，利用$$f_D$$计算出似然函数：

$$
L(\theta|x_1,x_2...x_n) = f_{\theta}(x_1,x_2,...x_n)
$$

从数学角度来说，我们尝试在$$\theta$$的所有取值中，找出一个值能够使似然函数最大化（使得获得该采样的可能性最大）。这个使可能性最大的$$\hat{\theta}$$，就称为$$\theta$$的最大似然估计。
在最大似然估计，有一个假设是观察得到的数据采样是独立同分布的，由于 IID 性质的存在，上面的联合概率可以变成一个**连乘**的式子。

$$
L(\theta|x_1, x_2, .. x_n) = \prod_{i=1}^{n}p_x(x_i;\theta)
$$

由于最大化一个似然函数和最大化其自然对数log是等价的。因为自然对数log是一个连续，并且在似然函数值域内严格递增的凸函数。求对数通过能够一定程度上简化计算量。

$$
Log(L(\theta|x_1, x_2, .. x_n)) = \sum_{i=1}^{n}p_x(x_i;\theta)
$$

### Likelihood Function for Logistic Regression

在LR中，对于每个训练数据$$(x_i, y_i)$$，对于每一类的概率，模型预测有以下情况：
* 如果 $$y_i = 1$$, 则概率为$$p_i$$;
* 如果 $$y_i = 0$$，则概率为$$1 - p_i$$

则模型的似然函数为：

$$
L(\mathbf{w}, b | \mathbf{X}) = \prod_{i=1}^{n}p(x_i)^{y_i}(1-p(x_i))^{1-y_i}
$$

取对数后进行得到 $$L(\mathbf{w}, b)$$。在这里，似然函数也算是LR的损失函数吧。

$$
\begin{equation}
\begin{aligned}
log(L(\mathbf{w}, b | \mathbf{X})) &= \sum_{i}^{n}y_ilogp(x_i) + (1 - y_i)log(1 - p(x_i)) \\
    &=  \sum_i^{n}log(1-p(x_i)) + \sum_{i}^{n}y_ilog\frac{p(x_i)}{1-p(x_i)} \\
    &=  \sum_i^{n}log(1-p(x_i)) + \sum_{i}^{n}y_i(\mathbf{w}^Tx_i + b) \\
    &=  \sum_i^{n}log\frac{1}{1 + \frac{1}{e^{-(w^Tx_i+b)}}} + \sum_{i}^{n}y_i(\mathbf{w}^Tx_i + b) \\
    &=  \sum_i^{n}-log(1 + e^{w^Tx_i + b}) + \sum_{i}^{n}y_i(\mathbf{w}^Tx_i + b) \\
\end{aligned}
\tag{3}\label{eq:3}
\end{equation}
$$

我们要最大化似然函数，那么顺利成章的，我们就对该似然函数进行求导并等于0。

$$
\begin{equation}
\begin{aligned}
\frac{\partial{l}}{\partial{\mathbf{w}}} &= -\sum_i^{n}\frac{1}{1 + e^{w^Tx_i + b}}e^{w^Tx_i + b} x_{i} + \sum_{i}^{n}y_ix_i \\
&= \sum_{i=1}^{n}(y_i - p(x_i; w, b)) x_{i}
\end{aligned}
\tag{4}\label{eq:4}
\end{equation}
$$

### 多分类问题

_上述讨论的似然函数，是关于二分类问题，也即是$$Y \in {0, 1}$$。对于多分类问题（样本数目为$$K$$），给定的训练样本中$$\mathbf{y_i} \in R^{\{K \times 1\}}$$的一个 01 向量。
我们一般采取 one vs all 的策略，针对每一类，我们会训练一个 LR 分类器（正例：$$y_i$$, 其余为反例）。因此最后会有 $$K$$ 个 二分类器。其中，我们定义第 $$k$$ 分类器的参数为$$\mathbf{w}_{(k)}, b_{(k)}$$。这也就是我们常说的 Softmax 回归：

$$
P(y = i | x; \mathbf{w}) = \frac{e^{\mathbf{w_i^T}x}}{\sum_{j=1}^{K}e^{\mathbf{w}_j^{T}x}}
$$

模型的输出，往往也是一个$$K$$维的向量$$\hat{y}_i \in R^{\{K \times 1\}}$$，向量的第 $$k$$ 元素表示样本属于第 $$k$$ 的概率值。回顾刚才的最大似然估计的定义，我们可以得出，对于单个样本$$x_i$$，其似然函数为：

$$
L(\mathbf{w} |  x_i) = \prod_{j=1}^{K}P(x=x_i|\mathbf{w_j})^{y_j^{(i)}}
$$

其中，$$y_j^{(i)}$$表示第$$i$$个样本的$$y$$向量中的第$$j$$个元素。在这里,$$y_j^{(i)}$$就好像一个指示灯（indicator variable），控制着那个类别概率的生效。考虑所有的训练样本，则得到多分类下的似然函数：

$$
\begin{equation}
\begin{aligned}
L(\mathbf{w} |  \mathbf{X}) &= \prod_{i=1}^{N}\prod_{j=1}^{K}P(x=x_i|\mathbf{w_j})^{y_j^{(i)}} \\
-Log(L(\mathbf{w} |  \mathbf{X})) &= -\sum_{i=1}^{N}\log(\prod_{j=1}^{K}P(x=x_i|\mathbf{w_j})^{y_j^{(i)}}) \\
                                 &= -\sum_{i=1}^{N}\sum_{j=1}^{K}y_j^{(i)}log(P(x=x_i|\mathbf{w_j}))
\end{aligned}
\tag{5}\label{eq:5}
\end{equation}
$$

<!--- 在上面的公式$$\eqref{eq:5}$$中，如果我们将里面的求和变成向量 $$\mathbf{y^{(i)}},log(P(x=x_i|\mathbf{w}))$$ 的内积，则即可得到大名鼎鼎的交叉熵损失函数： --->
在上面的公式中，如果我们将里面的求和变成向量 $$\mathbf{y^{(i)}},log(P(x=x_i|\mathbf{w}))$$ 的内积，则即可得到大名鼎鼎的交叉熵损失函数(Cross-Entropy Loss Function)：

$$
-Log(L(\mathbf{w} |  \mathbf{X})) = -\sum_{i=1}^{N}\mathbf{y^{(i)}log(P(x=x_i|\mathbf{w}))} 
$$

## 0x03. 解法

当我们把对数似然函数作为 LR 的损失函数，得到上面化简后的公式3。优化公式3有很多方法，常见的梯度下降法，牛顿法和拟牛顿法（BFGS）。优化的目标主要是在损失函数上找一个
下降的方向，参数跟随着这个方向迭代移动后就可以使得损失函数下降。这个方向往往是有一阶偏导或者二阶偏导数的组合而成。更详细的可以参考引用[3]。

在这里，简单的说下牛顿法。在实际优化问题时，牛顿法往往能够比梯度下降求解需要更少的迭代次数。是因为牛顿法是二阶收敛，而梯度下降是一阶收敛的。所以牛顿法就更快。如果更通俗地说的话，比如你想找一条最短的路径走到一个盆地的最底部，梯度下降法每次只从你当前所处位置选一个坡度最大的方向走一步，牛顿法在选择方向时，不仅会考虑坡度是否够大，还会考虑你走了一步之后，坡度是否会变得更大。所以，可以说牛顿法比梯度下降法看得更远一点，能更快地走到最底部。参考知乎回答[大饼土博 - 最优化问题中，牛顿法为什么比梯度下降法求解需要的迭代次数更少？](https://www.zhihu.com/question/19723347/answer/14636244)

## 0x04. 并行化

在工业届中，LR以实现简单、速度快、容易并行而广受欢迎。回顾整个LR的推导过程，可以发现LR的并行化其实主要是针对**目标函数梯度计算的并行化**。

在真实的应用中，样本的数量和特征数量往往都是非常巨大的。假设输入样本 $$X \in R^{\{M \times N\}}$$。可以发现在求导过程中，不可避免的就是矩阵的乘法$$W^TX$$，
因此，并行化主要是**将矩阵乘法进行分块并行计算后再归并汇总结果。主要体现在对$$X$$矩阵在列和行上的切割**。在参考引用[[3]](https://chenrudan.github.io/blog/2016/01/09/logisticregression.html)中有非常详细的说明，珠玉在前，我这里就不再献丑了。

![img](http://7xkmdr.com1.z0.glb.clouddn.com/lr6.jpg)

## 0x05. LR 与 非线性

从上面我们可以知道， LR 本质是广义线性模型的一种，其利用线性模型来逼近目标的对数几率。由于模型简单、易于并行、可解释性强等优点在CTR领域被广泛使用，一直是其中的benchmark模型。
但是，由于线性模型本身的局限，并不能处理特征和目标之间的非线性关系。因此我们常常需要一些特殊的手段，来让线性的模型学习到原始特征和目标之间的非线性关系。

要学习到非线性关系，常常需要我们对原始特征做一些非线性转化，而对于LR， 我们常见的转换方法就有 **连续特征离散化** 和 **特征交叉** 两种方法。

### 连续特征离散化

#### **离散特征**
在讨论连续特征之前，我们先来唠叨下『离散特征』。所谓的离散特征，在英文中一般称为 Category Feature，是指可枚举的特征类型。
例如二类别的性别特征（男/女），或者多类别的职业特征（学生/程序员/医生/老师）。在处理这些 Category Features 时，往往有两种处理方式：

1. 将离散的特征**Label化**，例如调用 `sklearn.preprocessing.LabelEncoder`，将特征转换为对应的类别ID。例如将特征列 `[学生, 学生, 程序员, 医生]` 转化为 `[0, 0, 1, 2]`
2. 将离散特征进行**OneHotEncode**。对于上面的例子，职业共有三种，则：
    * 职业`学生`则变为01向量：`[1, 0, 0]`
    * 职业`程序员`则变成了01向量：`[0, 1, 0]`
    * 职业`医生`则变成了01向量：`[0, 0, 1]`

对于第一种处理方式Label化，适合于能够直接处理Category Feature的模型，例如`XGBoost`，`LightGBM`，但是对于很多模型而言是不合适的。因为 Label ID 之间存在的一定的数理关系。例如`0 < 1`, `(0+2)/ 2 = 1`。这些数理关系在实际中都是没有意义的，甚至会对模型有伤害。

#### **连续特征**

为什么要将连续特征离散化，是因为对于 LR 这种线性模型而言，只能刻画特征和目标之间的线性关系。但是在实际中，例如年龄，并非年龄越大，对拟合目标的贡献就越大（线性关系），因此直接将年龄作为训练特征来说就不合适了。如果将特征年龄进行离散分段，每段相当于一个新的特征有属于自己的权重，就可以学习出一种类似『折线』的效果，也就是所谓的非线性结构：

<figure>
	<a href=""><img src="/assets/images/ctr_lr/discrete.png" alt="" width="500" heigh="300"></a>
</figure>

连续特征离散化的**方式**有很多，例如人为根据先验知识来进行切割，又或者有监督的训练单特征的决策树桩，根据基尼系数/信息增益来决定切割点的位置和个数。将连续特征切割成离散特征后，往往还会将特征进行 OnehotEncoding 化，得到一连串的 01 稀疏向量。

下面我们讲一个具体化的例子：假设有 Age 和 Income 两个连续变量。

|-------------+------------+--------------+---------------|
|   ID        |     Age    |   Income     |    ....       |
|-------------+------------+--------------+---------------|
|     1       |     27     |    800       |    ....       |
|     2       |     44     |    1000      |    ....       |
|     3       |     73     |    500       |    ....       |
|-------------+------------+--------------+---------------|

假如我们将年龄和Income都简单的离散化为3档后再做OneHotEncode，可以得到：

|-------------+------------+--------------+---------------+-----------------+-----------------+-------------+------|
|   ID        |     Age_0  |   Age_1      |     Age_2     |   Income_0      |   Income_1      |   Income_2  | .... |
|-------------+------------+--------------+---------------|-----------------+-----------------+-------------|------|
|     1       |     1      |    0         |     0         |      0          |        1        |     0       | .... |
|     2       |     0      |    1         |     0         |      0          |        0        |     1       | .... |
|     3       |     0      |    0         |     1         |      1          |        0        |     0       | .... |
|-------------+------------+--------------+---------------|-----------------+-----------------+-------------|------|


从上面无论是离散变量还是连续变量处理后，可以发现输入到 LR 模型的特征都是 **01的稀疏向量**，那么这样子做具有什么样的**优势**呢？

1. 离散特征的增加/减少都很方便，方便模型快速迭代。
2. 稀疏向量在存储上具有优势、而且稀疏向量的内积乘法有专门针对优化，速度很快。
3. 连续特征离散化后对模型更加稳定，例如对异常数据很强的鲁棒性，因为异常数据也仅仅是落入到其中的一个区间，微小的变化并不会造成模型很大的变化。
4. 离散化后，每个特征由属于自己新的权重，相当于为模型增加的非线性的能力，提升模型表达能力。加大拟合。
5. 离散化后可以进行特征交叉，由$$M+N$$个变量变为$$M \times N$$ 个变量，进一步引入非线性，提升表达能力；

### 特征交叉

在 LR 模型中，特征之间相互是呈现一个 IID 的关系，特征之间并没有太多相互之间的关系。而在类似决策树模型中，特征之间往往会有多阶交叉。如下图：

<figure>
	<a href=""><img src="/assets/images/ctr_lr/2.png" alt="" width="500" heigh="300"></a>
</figure>

单个特征往往对模型的预测能力比较弱，而不同类型的特征进行交叉后，可以产生一些有意义的新特征。例如
* 将特征中 『房子的长』 和 『房子的宽』做乘法，得到更有意义的『房子面积』这一维的特征。
* 将性别和购买类别特征进行交叉，可以得到“女性用户偏爱美妆类目”，“男性用户喜欢男装类目”的知识。
* ....

针对上面讨论离散化优点中的第四点，『离散化后可以进行特征交叉，由$$M+N$$个变量变为$$M \times N$$ 个变量，进一步引入非线性』，可以理解为离散化后分别得到一个$$M,N$$维的01向量，例如不同位置的元素交叉分别做**与/或**操作后（笛卡尔乘积），可以得到一个$$M \times N$$的新特征向量。其具体的业务意义，可以自己理解哈。

但是，这种人工的特征工程往往会耗费大量的精力，同时也需要对业务知识有着充分的把握，即使有经验的工程师有时候也难以发掘完所有有用的特征。

## 0x06. 引用

* [1]. [CMU Lecture Notes: Chapter 12 Logistic Regression](http://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch12.pdf)
* [2]. [Machine Learning 机器学习，周志华]()
* [3]. [https://chenrudan.github.io/blog/2016/01/09/logisticregression.html](【机器学习算法系列之二】浅析Logistic Regression)
* [4]. [知乎：大饼土博 - 最优化问题中，牛顿法为什么比梯度下降法求解需要的迭代次数更少？](https://www.zhihu.com/question/19723347/answer/14636244)

